{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi_Fare_Prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNqLZhQ4R1xA/6V3/bCONwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahassanzadeh/Taxi_Fare_Prediction/blob/main/Taxi_Fare_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1hruO7b_5Am"
      },
      "source": [
        "# loading data to google ceolab\n",
        "# !wget -O train.csv https://www.dropbox.com/s/mnty1y72gweqjj1/train.csv?dl=0\n",
        "# !wget -O test.csv https://www.dropbox.com/s/7cvc0s50u9350lo/test.csv?dl=0\n",
        "# !wget -O sammple_submission.csv https://www.dropbox.com/s/euh08kcj7khs89b/sample_submission.csv?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2d3medyUTmX"
      },
      "source": [
        "# Visulation package \n",
        "# !pip install folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUIHC8fVh3Ms"
      },
      "source": [
        "# install bayesian-optimization package \n",
        "# !pip install bayesian-optimization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxvR93IEUxfc"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n",
        "# from mpl_toolkits.basemap import Basemap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5964dmsr3oga"
      },
      "source": [
        "%%time\n",
        "# To reduce the computation and memory allocation, just read 2M rows initially \n",
        "df_train= pd.read_csv('train.csv', nrows=2000000)\n",
        "df_test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EVah7bLHq3E"
      },
      "source": [
        "# tranform object to datetime format \n",
        "df_train['pickup_datetime'] = df_train['pickup_datetime'].str.slice(0, 16)\n",
        "df_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5TvU43IebS"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3pZilY_KWVy"
      },
      "source": [
        "df_train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxrh1GnKI3TA"
      },
      "source": [
        "df_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRc5PvfJJ5jZ"
      },
      "source": [
        "Describe function indicate that there is negetive amount for fares, so it has to be corrected! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5bKsGjMN_1F"
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlx7SjIWJ4sM"
      },
      "source": [
        "df_train = df_train[df_train.fare_amount>=0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4wmYJMdJjeL"
      },
      "source": [
        "# plot histogram of fare\n",
        "plt.figure(figsize=(20, 3))\n",
        "df_train[df_train.fare_amount<60].fare_amount.hist(bins=60)\n",
        "plt.xlabel('Taxi Fare ($)')\n",
        "plt.title('Histogram');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eigXFN0vO7UL"
      },
      "source": [
        "There is some unexpected price increase at $45, 50, 55 that has to be investigated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6Y02EGOP7i"
      },
      "source": [
        "# remove any possible null rows in the dataset \n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eACMUy0MTNJT"
      },
      "source": [
        "# Location Data \n",
        "As pickup and dropoff location is critical, it is necessary to visulize the location for train and test data, find any outlier and whether their effect on prediction is dominant! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eie5sWNCaTor"
      },
      "source": [
        "# first finding the maximum and minimum longtitude and latitude in the test dataset \n",
        "print('maximum latitude is ', max(df_test.pickup_latitude.max(), df_test.dropoff_latitude.max()))\n",
        "print('minimum latitude is ', min(df_test.pickup_latitude.min(), df_test.dropoff_latitude.min()))\n",
        "print('maximum longitude is ', max(df_test.pickup_longitude.max(), df_test.dropoff_longitude.max()))\n",
        "print('minimum longitude is ', min(df_test.pickup_longitude.min(), df_test.dropoff_longitude.min()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP9TtiYUaXRO"
      },
      "source": [
        "import folium\n",
        "from folium.plugins import FastMarkerCluster\n",
        "\n",
        "def plot_map(df, maxpoints=len(df_test)):\n",
        "    map = folium.Map( location=[ df[\"pickup_latitude\"].median(), df[\"pickup_longitude\"].median()], width ='90%', height='90%', zoom_start=10)\n",
        "\n",
        "    for index, row in enumerate(list(zip(df[\"pickup_latitude\"].values, df[\"pickup_longitude\"].values))):\n",
        "        folium.CircleMarker(location=row, radius=2, weight=1, color='green').add_to(map)\n",
        "        if index == maxpoints:\n",
        "            break\n",
        "\n",
        "    for index, row in enumerate(list(zip(df[\"dropoff_latitude\"].values, df[\"dropoff_longitude\"].values))):\n",
        "        folium.CircleMarker(location=row, radius=2, weight=1, color='red').add_to(map)\n",
        "        if index == maxpoints:\n",
        "            break     \n",
        "            \n",
        "    return map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itKTn3H4jVZq"
      },
      "source": [
        "plot_map(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BfVDkwQejsG"
      },
      "source": [
        "plot_map(df_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Jp18OSk3ee"
      },
      "source": [
        "As the visulation of data for train and datasets shows, the distribution of data for location are from similar distribution. Also there are minorites of outliers and wrong location data(in the water), however as the very few of these data, it is ignoreable at this stage! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa9ZCWlwqJA6"
      },
      "source": [
        "# Feature Engineering \n",
        "\n",
        "We add features for distance and NYC Taxis regulation:\n",
        "## There are additional regulations for Taxis in NYC:\n",
        "\n",
        "- Initial charge for most rides (excluding from JFK and other airports) is 2.5 dollars upon entry. After that there \\$0.5 every unit where the unit is defined as 1/5th of a mile or when the Taxicab is traveling 12 Miles an hour or more.\n",
        "- \\$0.5 of additional surcharge between 8PM - 6AM.\n",
        "- Peak hour weekday surcharge of \\$1 Monday-Friday between 4PM-8PM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me4_AI2NqcEV"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhc282nLekfE"
      },
      "source": [
        "# Removing observations with erroneous values\n",
        "limit = df_train['pickup_longitude'].between(-75, -73)\n",
        "limit &= df_train['dropoff_longitude'].between(-75, -73)\n",
        "limit &= df_train['pickup_latitude'].between(40, 42)\n",
        "limit &= df_train['dropoff_latitude'].between(40, 42)\n",
        "limit &= df_train['passenger_count'].between(0, 8)\n",
        "limit &= df_train['fare_amount'].between(0, 250)\n",
        "\n",
        "df = df_train[limit]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8IASgUwqX-1"
      },
      "source": [
        "# Distance feature \n",
        "# the best practice to find road distance of two location, is using google distance matrix api (https://developers.google.com/maps/documentation/distance-matrix/overview)\n",
        "# to find exact distance, however as it is not free, i just use the next best thing, which is using Manhattan distance. \n",
        "# round up distance to 2 digits as less than 0.01 mile is negligible distance \n",
        "\n",
        "def distance_func(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n",
        "    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n",
        "\n",
        "    return round(distance,2)\n",
        "\n",
        "# feature engineer the passenger trip distance and also the distance to 3 major airports in NewYork in order consider the airport effect on the fare forcasting\n",
        "\n",
        "def distance_features(data):\n",
        "\n",
        "    # Extract date attributes and then drop the pickup_datetime column\n",
        "    data['hour'] = data['pickup_datetime'].dt.hour\n",
        "    data['day'] = data['pickup_datetime'].dt.day\n",
        "    data['weekday'] = data['pickup_datetime'].dt.weekday\n",
        "    data['month'] = data['pickup_datetime'].dt.month\n",
        "    data['year'] = data['pickup_datetime'].dt.year\n",
        "    data = data.drop('pickup_datetime', axis=1)\n",
        "\n",
        "    # Longtitue and Latitude of city center and nearby airports\n",
        "    NewYork = (-74.0063889, 40.72)\n",
        "    JFK_airport = (-73.7822222222, 40.64)\n",
        "    Neward_airport = (-74.175, 40.69)\n",
        "    Laguardia_airport = (-73.87, 40.77)\n",
        "\n",
        "\n",
        "    # Adding feature columns \n",
        "    data['distance'] = distance_func(data['pickup_latitude'], data['pickup_longitude'], data['dropoff_latitude'], data['dropoff_longitude'])\n",
        "\n",
        "    data['distance_to_center_NewYork'] = distance_func(NewYork[1], NewYork[0],\n",
        "                                          data['pickup_latitude'], data['pickup_longitude'])\n",
        "    data['pickup_distance_to_JFK_airport'] = distance_func(JFK_airport[1], JFK_airport[0],\n",
        "                                         data['pickup_latitude'], data['pickup_longitude'])\n",
        "    data['dropoff_distance_to_JFK_airport'] = distance_func(JFK_airport[1], JFK_airport[0],\n",
        "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
        "    data['pickup_distance_to_Neward_airport'] = distance_func(Neward_airport[1], Neward_airport[0], \n",
        "                                          data['pickup_latitude'], data['pickup_longitude'])\n",
        "    data['dropoff_distance_to_Neward_airport'] = distance_func(Neward_airport[1], Neward_airport[0],\n",
        "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
        "    data['pickup_distance_to_Laguardia_airport'] = distance_func(Laguardia_airport[1], Laguardia_airport[0],\n",
        "                                          data['pickup_latitude'], data['pickup_longitude'])\n",
        "    data['dropoff_distance_to_Laguardia_airport'] = distance_func(Laguardia_airport[1], Laguardia_airport[0],\n",
        "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
        "    \n",
        "    data['long_dist'] = abs(data['pickup_longitude'] - data['dropoff_longitude'])\n",
        "    data['lat_dist'] = abs(data['pickup_latitude'] - data['dropoff_latitude'])\n",
        "    \n",
        "    data = data.dropna(how = 'any', axis = 'rows')\n",
        "\n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gzJGZUk-Z-L"
      },
      "source": [
        "# Adding distance features to the dataset  \n",
        "df_train_new = distance_features(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f8WAkRVrUgJ"
      },
      "source": [
        "# remove datapoints with distance less than 0.01 miles(too close)\n",
        "idx = (df_train_new['distance'] >= 0.01)\n",
        "df_train_new['distance'] = df_train_new['distance'][idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKRzrYMOBjqL"
      },
      "source": [
        "# adding time features\n",
        "\n",
        "# one-hot encoding the 8PM-6PM \n",
        "df_train_new['daily_subcharge'] =  np.zeros((len(df_train_new), 1)).astype('int')\n",
        "idx_hour = df_train_new[(df_train_new['hour'] >= 20) | (df_train_new['hour'] <= 6)]['hour']\n",
        "df_train_new['daily_subcharge'][idx_hour.index] = 1\n",
        "\n",
        "# one-hot encoding the Peak hour weekday surcharge of $1 Monday-Friday between 4PM-8PM.\n",
        "df_train_new['weekday_subcharge'] =  np.zeros((len(df_train_new), 1)).astype('int')\n",
        "idx_day = df_train_new[((df_train_new['hour'] >= 16) & (df_train_new['hour'] <= 20)) & (((df_train_new['day'] >= 0) & (df_train_new['day'] <= 4)))]['day']\n",
        "df_train_new['weekday_subcharge'][idx_day.index] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psPeOI3aXHqk"
      },
      "source": [
        "# Training and Cross Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K2liUcwXy9m"
      },
      "source": [
        "# Training library\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ee_pLeaXLz7"
      },
      "source": [
        "X, y = df_train_new.drop('fare_amount', axis = 1), df_train_new['fare_amount']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_irLdJgvS3mu"
      },
      "source": [
        "# find correlation of training data \n",
        "plt.figure(figsize = (20, 10))\n",
        "sns.heatmap(X.corr(), annot = True, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxceUgAbgFKG"
      },
      "source": [
        "df_train_new = df_train_new.drop(['key'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUYU0atg3JM"
      },
      "source": [
        "df_train_new.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR1VryAuNuuA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_train_new.drop('fare_amount', axis=1), df_train_new['fare_amount'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrXOJC-UYL_o"
      },
      "source": [
        "del(df_train_new)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "del(X_train)\n",
        "dtest = xgb.DMatrix(X_test)\n",
        "del(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azfBlGFwiLyj"
      },
      "source": [
        "def evaluate(max_depth, gamma, colsample_bytree):\n",
        "    params = {'eval_metric': 'rmse',\n",
        "              'max_depth': int(max_depth),\n",
        "              'subsample': 0.8,\n",
        "              'eta': 0.1,\n",
        "              'gamma': gamma,\n",
        "              'colsample_bytree': colsample_bytree}\n",
        "    # Used around 1000 boosting rounds in the full model\n",
        "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
        "    \n",
        "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
        "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgSciuS7ZFLg"
      },
      "source": [
        "xgb_boost = BayesianOptimization(evaluate, {'max_depth': (3, 7), \n",
        "                                             'gamma': (0, 1),\n",
        "                                             'colsample_bytree': (0.3, 0.9)})\n",
        "xgb_boost.maximize(init_points=3, n_iter=5, acq='ei')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2FspkCMhHTE"
      },
      "source": [
        "# Extract the parameters of the best model for training xgboost.\n",
        "params = xgb_bo.res[3]['params']\n",
        "params['max_depth'] = int(params['max_depth'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XRdIeVnoTLz"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB12jxg5oUQ3"
      },
      "source": [
        "# Train a new model with the best parameters from the search\n",
        "model_xgboost = xgb.train(params, dtrain, num_boost_round=250)\n",
        "\n",
        "# Predict on testing and training set\n",
        "y_pred = model_xgboost.predict(dtest)\n",
        "y_train_pred = model_xgboost.predict(dtrain)\n",
        "\n",
        "# Report testing and training RMSE\n",
        "print(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(np.sqrt(mean_squared_error(y_train, y_train_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiREeAUmoXYl"
      },
      "source": [
        "#EVALUATION OF THE MODEL\n",
        "# Plotting y_test and y_pred to understand the spread.\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.scatter(y_test,y_pred)\n",
        "fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \n",
        "plt.xlabel('y_test', fontsize=18)                          # X-label\n",
        "plt.ylabel('y_pred', fontsize=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsK1rkZXo1Wd"
      },
      "source": [
        "# Feature Importance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I2ZwqRZo0yJ"
      },
      "source": [
        "fscores = pd.DataFrame({'X': list(model_xgboost.get_fscore().keys()), 'Y': list(model_xgboost.get_fscore().values())})\n",
        "fscores.sort_values(by='Y').plot.bar(x='X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR3CjowopCde"
      },
      "source": [
        "# Predict on the given test dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9bjSa5woz8X"
      },
      "source": [
        "test = pd.read_csv('test.csv').set_index('key')\n",
        "test['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)\n",
        "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "\n",
        "# Predict on holdout set\n",
        "test = distance_features(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3XLZyo5emq"
      },
      "source": [
        "# adding time features\n",
        "\n",
        "# one-hot encoding the 8PM-6PM \n",
        "test['daily_subcharge'] =  np.zeros((len(test), 1)).astype('int')\n",
        "idx_hour = test[(test['hour'] >= 20) | (test['hour'] <= 6)]['hour']\n",
        "test['daily_subcharge'][idx_hour.index] = 1\n",
        "\n",
        "# one-hot encoding the Peak hour weekday surcharge of $1 Monday-Friday between 4PM-8PM.\n",
        "test['weekday_subcharge'] =  np.zeros((len(test), 1)).astype('int')\n",
        "idx_day = test[((test['hour'] >= 16) & (test['hour'] <= 20)) & (((test['day'] >= 0) & (test['day'] <= 4)))]['day']\n",
        "test['weekday_subcharge'][idx_day.index] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uNlfx8H5fHz"
      },
      "source": [
        "dtest = xgb.DMatrix(test)\n",
        "y_pred_test = model_xgboost.predict(dtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWNyGeREpGp5"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmaROVRapHjc"
      },
      "source": [
        "holdout = pd.DataFrame({'key': test.index, 'fare_amount': y_pred_test})\n",
        "holdout.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}